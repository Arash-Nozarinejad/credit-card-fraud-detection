{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eddb433-2e58-4b04-a66c-102c0dd8e08c",
   "metadata": {},
   "source": [
    "# Step 9 - Preprocessing pipeline\n",
    "\n",
    "Not much preprocessing is needed for this project. Previous steps have shown that # Code for 9.3y. However, step 7 showed the Amount feature can benefit from log transformation and thus we'll include that in our pipeline.\n",
    "\n",
    "In this step I will also perform the train/test/validate split and save the data and the pipeline for future steps and use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44882920-9859-4f52-b84d-e8412bfecdb7",
   "metadata": {},
   "source": [
    "## 9.1 Data Splitting\n",
    "\n",
    "Before we create the pipeline we have to split the data to avoid data leakage. To preserve the class imbalance, stratified data splitting must be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2735531-1762-401b-a37f-764ef1786fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 284,807 samples\n",
      "Fraud rate: 0.173%\n",
      "\n",
      "Data splits:\n",
      "Training: 199,364 samples (70.0%)\n",
      "Validation: 42,721 samples (15.0%)\n",
      "Test: 42,722 samples (15.0%)\n",
      "\n",
      "Fraud rates:\n",
      "Training: 0.173%\n",
      "Validation: 0.173%\n",
      "Test: 0.173%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "# Load data\n",
    "data_path = Path(\"../data/creditcard.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "print(f\"Original dataset: {len(df):,} samples\")\n",
    "print(f\"Fraud rate: {y.mean():.3%}\")\n",
    "\n",
    "# First split: 70% train, 30% temp (which we'll split into 15% val + 15% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Second split: Split the 30% temp into 15% validation and 15% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5,  # 0.5 of 30% = 15% each\n",
    "    random_state=42, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Confirm splits\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"Training: {len(X_train):,} samples ({len(X_train)/len(df):.1%})\")\n",
    "print(f\"Validation: {len(X_val):,} samples ({len(X_val)/len(df):.1%})\")\n",
    "print(f\"Test: {len(X_test):,} samples ({len(X_test)/len(df):.1%})\")\n",
    "\n",
    "# Verify fraud rates are preserved\n",
    "print(f\"\\nFraud rates:\")\n",
    "print(f\"Training: {y_train.mean():.3%}\")\n",
    "print(f\"Validation: {y_val.mean():.3%}\")\n",
    "print(f\"Test: {y_test.mean():.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591971a6-20d8-46a6-90d9-4e4c9576c9ef",
   "metadata": {},
   "source": [
    "## 9.2 Pipeline Creation\n",
    "\n",
    "Now the pipeline can be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81afb150-f318-4af6-b0c2-aff17ee2548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline created\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "\n",
    "def log_transform_amount(X):\n",
    "    \"\"\"Apply log1p transformation to Amount column\"\"\"\n",
    "    X_transformed = X.copy()\n",
    "    X_transformed['Amount'] = np.log1p(X_transformed['Amount'])\n",
    "    return X_transformed\n",
    "\n",
    "# Create preprocessing pipeline. We only transform Amount since PCA features are already standardized\n",
    "preprocessing_pipeline = FunctionTransformer(\n",
    "    func=log_transform_amount,\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipeline created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c96f0-e402-4c8d-b948-4b08ed4b3c40",
   "metadata": {},
   "source": [
    "## 9.3 Pipeline Testing & Validation\n",
    "\n",
    "Before saving the pipeline it's important to test and validate if the pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82df9c9f-b059-4cb0-a337-1458ac6cc63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessing pipeline...\n",
      "\n",
      "Amount transformation verification:\n",
      "Original Amount range: $0.00 - $25691.16\n",
      "Transformed Amount range: 0.000 - 10.154\n",
      "\n",
      "PCA feature (V1) unchanged verification:\n",
      "Original: -0.0011 ± 1.9658\n",
      "Processed: -0.0011 ± 1.9658\n",
      "\n",
      "End-to-end pipeline test:\n",
      "Training data shape: (199364, 30)\n",
      "Validation data shape: (42721, 30)\n",
      "Test data shape: (42722, 30)\n"
     ]
    }
   ],
   "source": [
    "# Test pipeline on training data\n",
    "print(\"Testing preprocessing pipeline...\")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_processed = preprocessing_pipeline.fit_transform(X_train)\n",
    "\n",
    "# Transform validation and test data (using fitted pipeline)\n",
    "X_val_processed = preprocessing_pipeline.transform(X_val)\n",
    "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "# Verify transformations worked correctly\n",
    "print(f\"\\nAmount transformation verification:\")\n",
    "print(f\"Original Amount range: ${X_train['Amount'].min():.2f} - ${X_train['Amount'].max():.2f}\")\n",
    "print(f\"Transformed Amount range: {X_train_processed['Amount'].min():.3f} - {X_train_processed['Amount'].max():.3f}\")\n",
    "\n",
    "# Check that other features are unchanged\n",
    "pca_feature = 'V1'\n",
    "print(f\"\\nPCA feature ({pca_feature}) unchanged verification:\")\n",
    "print(f\"Original: {X_train[pca_feature].mean():.4f} ± {X_train[pca_feature].std():.4f}\")\n",
    "print(f\"Processed: {X_train_processed[pca_feature].mean():.4f} ± {X_train_processed[pca_feature].std():.4f}\")\n",
    "\n",
    "# End-to-end test\n",
    "print(f\"\\nEnd-to-end pipeline test:\")\n",
    "print(f\"Training data shape: {X_train_processed.shape}\")\n",
    "print(f\"Validation data shape: {X_val_processed.shape}\")\n",
    "print(f\"Test data shape: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93ce9de-a4ca-4073-9955-b294a69da983",
   "metadata": {},
   "source": [
    "## 9.4 Save Everything\n",
    "\n",
    "Finally the pipeline and split data need to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ae63502-4802-4fe4-8d14-31889013b3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline saved\n",
      "\n",
      "Saving processed data splits...\n",
      "Training data: 199,364 samples\n",
      "Validation data: 42,721 samples\n",
      "Test data: 42,722 samples\n",
      "\n",
      "Testing pipeline loading...\n",
      "Pipeline loads and transforms correctly\n"
     ]
    }
   ],
   "source": [
    "# Create directories if they don't exist\n",
    "Path(\"../models\").mkdir(exist_ok=True)\n",
    "Path(\"../data/splits\").mkdir(exist_ok=True)\n",
    "\n",
    "# Save preprocessing pipeline\n",
    "pipeline_path = \"../models/preprocessing_pipeline.pkl\"\n",
    "joblib.dump(preprocessing_pipeline, pipeline_path)\n",
    "print(f\"Preprocessing pipeline saved\")\n",
    "# print(f\"Preprocessing pipeline saved to: {pipeline_path}\")\n",
    "\n",
    "# Save processed data splits\n",
    "print(f\"\\nSaving processed data splits...\")\n",
    "\n",
    "# Save processed features\n",
    "X_train_processed.to_csv(\"../data/splits/X_train_processed.csv\", index=False)\n",
    "X_val_processed.to_csv(\"../data/splits/X_val_processed.csv\", index=False)\n",
    "X_test_processed.to_csv(\"../data/splits/X_test_processed.csv\", index=False)\n",
    "\n",
    "# Save targets\n",
    "y_train.to_csv(\"../data/splits/y_train.csv\", index=False)\n",
    "y_val.to_csv(\"../data/splits/y_val.csv\", index=False)\n",
    "y_test.to_csv(\"../data/splits/y_test.csv\", index=False)\n",
    "\n",
    "print(f\"Training data: {X_train_processed.shape[0]:,} samples\")\n",
    "print(f\"Validation data: {X_val_processed.shape[0]:,} samples\") \n",
    "print(f\"Test data: {X_test_processed.shape[0]:,} samples\")\n",
    "\n",
    "# Test loading pipeline (verification)\n",
    "print(f\"\\nTesting pipeline loading...\")\n",
    "loaded_pipeline = joblib.load(pipeline_path)\n",
    "test_transform = loaded_pipeline.transform(X_train.head(5))\n",
    "print(f\"Pipeline loads and transforms correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d05cf5b-3d6b-4ba6-a7db-a3f4bf7a0472",
   "metadata": {},
   "source": [
    "## Step 9 - Conclusion\n",
    "\n",
    "The preprocessing pipeline has been successfully created and validated. Since our data was already high quality from the Kaggle dataset, minimal preprocessing was required:\n",
    "\n",
    "* **Data splitting**: 70/15/15 train/validation/test split with stratification to preserve the fraud class balance\n",
    "* **Amount transformation**: Log transformation applied to handle the right-skewed distribution identified in Step 7\n",
    "* **Pipeline validation**: Confirmed transformations work correctly with no data leakage\n",
    "\n",
    "The pipeline and split datasets are now saved and ready for model training in Step 10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
