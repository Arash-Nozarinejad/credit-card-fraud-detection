{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c80c1e-0f76-4d63-8783-07672a2c3474",
   "metadata": {},
   "source": [
    "# Step 8 - Baseline Model Establishment\n",
    "\n",
    "It's always a good practice to establish a baseline for how well the simplest approach performs on the data. This way we'll know if implementing changes and using sophisticated models is justified.\n",
    "\n",
    "This is an important step in which:\n",
    "\n",
    "1. I'll know the minimum acceptable performance.\n",
    "2. I can evaluate if the problem is easy or hard.\n",
    "3. Maybe simple rules perform well and there's no need to over complicate things.\n",
    "4. Establish business value for trying complex approaches over simple ones if needed.\n",
    "\n",
    "For our classification problem with an imbalanced dataset I'll establish a number of baselines:\n",
    "\n",
    "* Always predicting the majority class\n",
    "* Randomly predicting with and without class probability\n",
    "* Using simple if-then statements based on the information from the previous steps.\n",
    "* Predicting using only a single Feature (Amount).\n",
    "* Predicting using the default scikit-learn models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53994bc-cd11-4b7c-b396-d6cfa7d51af8",
   "metadata": {},
   "source": [
    "## 8.1 Majority Class Prediction Results\n",
    "\n",
    "It's necessary to have a baseline of evaluation metrics when only the majority class is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e29bef6-8136-4efd-98e1-0fb833221343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "8.1 Majority Class Prediction Results\n",
      "Training set: 227,845 samples\n",
      "Test set: 56,962 samples\n",
      "Fraud rate in test: 0.002%\n",
      "\n",
      "Baseline Results:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.000\n",
      "Accuracy: 0.998\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "data_path = Path(\"../data/creditcard.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Prepare features\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 8.1 MAJORITY CLASS BASELINE\n",
    "# Always predict the most common class (0 = legitimate)\n",
    "majority_pred = np.zeros(len(y_test)) # Predict all as legitimate (0)\n",
    "\n",
    "# Calculate metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, majority_pred, average='binary')\n",
    "\n",
    "print(\"=\"*25)\n",
    "print(\"8.1 Majority Class Prediction Results\")\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "print(f\"Fraud rate in test: {y_test.mean():.3f}%\")\n",
    "\n",
    "print(\"\\nBaseline Results:\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(f\"Accuracy: {(majority_pred == y_test).mean():.3f}\")\n",
    "print(\"=\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe84c4d-d59c-4918-9eeb-5b11b79e4e40",
   "metadata": {},
   "source": [
    "### 8.1 Results\n",
    "\n",
    "As expected, predicting only the majority class gives us perfect accuracy (99.8%) but completely useless precision and recall (0%). This confirms what we learned in Step 2 - accuracy is meaningless for fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cec053c-3dc9-4dbe-b9e6-26b3daaeaf16",
   "metadata": {},
   "source": [
    "## 8.2 Random Prediction Results\n",
    "\n",
    "Sometimes even complex models perform no better than a model predicting by random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6e49b29-7394-46ff-ad4a-1d36906abb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "8.2 Random Prediction Results\n",
      "\n",
      "Method 1 - Random with true fraud rate (0.002):\n",
      "  Precision: 0.000\n",
      "  Recall: 0.000\n",
      "  F1-Score: 0.000\n",
      "  Accuracy: 0.997\n",
      "\n",
      "Method 2 - Random 50/50 predictions:\n",
      "  Precision: 0.002\n",
      "  Recall: 0.500\n",
      "  F1-Score: 0.003\n",
      "  Accuracy: 0.500\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*25)\n",
    "print(\"8.2 Random Prediction Results\")\n",
    "\n",
    "# Use training set to get true fraud rate\n",
    "fraud_rate = y_train.mean()\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Method 1: Random with true class probability\n",
    "np.random.seed(42)\n",
    "random_pred_realistic = np.random.binomial(1, fraud_rate, size=len(y_test))\n",
    "\n",
    "# Method 2: Random with 50/50 probability  \n",
    "np.random.seed(42)\n",
    "random_pred_50_50 = np.random.binomial(1, 0.5, size=len(y_test))\n",
    "\n",
    "# Calculate metrics for both\n",
    "precision1, recall1, f1_1, _ = precision_recall_fscore_support(y_test, random_pred_realistic, average='binary')\n",
    "precision2, recall2, f1_2, _ = precision_recall_fscore_support(y_test, random_pred_50_50, average='binary')\n",
    "\n",
    "print(f\"\\nMethod 1 - Random with true fraud rate ({fraud_rate:.3f}):\")\n",
    "print(f\"  Precision: {precision1:.3f}\")\n",
    "print(f\"  Recall: {recall1:.3f}\")\n",
    "print(f\"  F1-Score: {f1_1:.3f}\")\n",
    "print(f\"  Accuracy: {(random_pred_realistic == y_test).mean():.3f}\")\n",
    "\n",
    "print(f\"\\nMethod 2 - Random 50/50 predictions:\")\n",
    "print(f\"  Precision: {precision2:.3f}\")\n",
    "print(f\"  Recall: {recall2:.3f}\")\n",
    "print(f\"  F1-Score: {f1_2:.3f}\")\n",
    "print(f\"  Accuracy: {(random_pred_50_50 == y_test).mean():.3f}\")\n",
    "\n",
    "print(\"=\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f3435d-340d-4679-ae4b-95f087a05918",
   "metadata": {},
   "source": [
    "### 8.2 Results\n",
    "\n",
    "Random predictions show us two important lessons:\n",
    "\n",
    "1. Using the true fraud rate (0.2%) for random predictions gives us the same useless results as majority class - essentially no fraud detection.\n",
    "2. Randomly flagging half of all transactions gives us 50% recall (catches half the fraud) but terrible precision (0.2%). This means we'd flag ~28,500 legitimate transactions to catch ~50 fraud cases - completely unacceptable for business use.\n",
    "\n",
    "This shows the classic precision-recall trade-off. You can always increase recall by flagging more transactions, but precision suffers dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ab493-d4bf-4ad7-a269-eb4b2f95f875",
   "metadata": {},
   "source": [
    "## 8.3 Rule-based prediction\n",
    "\n",
    "Sometimes combining EDA with domain knowledge yields to decent prediction rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d711833-e531-4950-8405-bce3af676b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "8.3 Rule-based prediction\n",
      "Precision: 0.004\n",
      "Recall: 0.051\n",
      "F1-Score: 0.007\n",
      "Accuracy: 0.975\n",
      "Flagged transactions: 1,337 out of 56,962\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*25)\n",
    "print(\"8.3 Rule-based prediction\")\n",
    "\n",
    "def simple_fraud_rules(amount):\n",
    "    \"\"\"\n",
    "    Simple rules based on EDA findings:\n",
    "    - Higher fraud rate in $500-1000 range\n",
    "    \"\"\"\n",
    "    # Rule 1: Flag high-value transactions\n",
    "    if amount >= 500 and amount <= 1000:\n",
    "        return 1\n",
    "    \n",
    "    # Default: legitimate\n",
    "    return 0\n",
    "\n",
    "rule_pred = [simple_fraud_rules(amount) for amount in X_test['Amount']]\n",
    "\n",
    "# Calculate metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, rule_pred, average='binary')\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(f\"Accuracy: {(np.array(rule_pred) == y_test).mean():.3f}\")\n",
    "print(f\"Flagged transactions: {sum(rule_pred):,} out of {len(rule_pred):,}\")\n",
    "\n",
    "print(\"=\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b8e83-abc3-43f7-838b-c4b139d72676",
   "metadata": {},
   "source": [
    "### 8.3 Results\n",
    "\n",
    "The rule-based approach catches 5.1% of fraud cases but with a precession of 0.4% which means 1,337 transactions were flagged for justa few frauds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada2130-8e82-4b0c-9f55-e7a99a5a461d",
   "metadata": {},
   "source": [
    "## 8.4 Single Feature Prediction\n",
    "\n",
    "In this project we know Amount is a good indicator of fraud and thus we need to know the baseline metrics using a single feature when we know we have a feature with high correlation with the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0627c41b-a8cd-4266-bcc2-3d8569fb99b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "8.4 Single Feature Prediction\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.000\n",
      "Accuracy: 0.998\n",
      "Model coefficient: 0.0002\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"=\"*25)\n",
    "print(\"8.4 Single Feature Prediction\")\n",
    "\n",
    "# Prepare single feature\n",
    "X_train_amount = X_train[['Amount']]\n",
    "X_test_amount = X_test[['Amount']]\n",
    "\n",
    "# Simple logistic regression with one feature\n",
    "single_feature_model = LogisticRegression(random_state=42)\n",
    "single_feature_model.fit(X_train_amount, y_train)\n",
    "\n",
    "# Make predictions\n",
    "single_pred = single_feature_model.predict(X_test_amount)\n",
    "\n",
    "# Calculate metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, single_pred, average='binary')\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(f\"Accuracy: {(single_pred == y_test).mean():.3f}\")\n",
    "\n",
    "# Show what the model learned\n",
    "coef = single_feature_model.coef_[0][0]\n",
    "intercept = single_feature_model.intercept_[0]\n",
    "print(f\"Model coefficient: {coef:.4f}\")\n",
    "\n",
    "print(\"=\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd918660-0515-4413-9b4c-d821525eca5c",
   "metadata": {},
   "source": [
    "### 8.4 Results\n",
    "\n",
    "Single feature prediction with Amount fails completely, despite Amount showing promise in our EDA. The positive coefficient confirms higher amounts correlate with fraud, but the signal is too weak when used alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036f86f-13be-4c56-ae78-de3eee33920a",
   "metadata": {},
   "source": [
    "## 8.5 Default Models Performance\n",
    "\n",
    "Before applying feature engineering or hyperparameter tuning, knowing how each default model perform on the data is crucial.\n",
    "\n",
    "For this project we'll be using:\n",
    "\n",
    "* **Logistic Regression** - Simple linear baseline\n",
    "* **Random Forest** - Tree-based method that handles features well\n",
    "* **XGBoost** - Popular approach for tabular data\n",
    "\n",
    "To establish baseline for the different approaches (linear, bagging, boosting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a42725-0ae2-4f87-bd00-4f0943cd7aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "8.5 Default Models Performance\n",
      "\n",
      "Logistic Regression:\n",
      "  Precision: 0.827\n",
      "  Recall: 0.633\n",
      "  F1-Score: 0.717\n",
      "  Accuracy: 0.999\n",
      "\n",
      "Random Forest:\n",
      "  Precision: 0.941\n",
      "  Recall: 0.816\n",
      "  F1-Score: 0.874\n",
      "  Accuracy: 1.000\n",
      "\n",
      "XGBoost:\n",
      "  Precision: 0.867\n",
      "  Recall: 0.796\n",
      "  F1-Score: 0.830\n",
      "  Accuracy: 0.999\n",
      "\n",
      "Baseline Summary:\n",
      "============================================================\n",
      "Model                Precision  Recall     F1        \n",
      "============================================================\n",
      "Logistic Regression  0.827      0.633      0.717     \n",
      "Random Forest        0.941      0.816      0.874     \n",
      "XGBoost              0.867      0.796      0.830     \n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=\"*25)\n",
    "print(\"8.5 Default Models Performance\")\n",
    "\n",
    "# Scale features for logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Use scaled data for logistic regression, original for tree-based\n",
    "    if 'Logistic' in name:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, pred, average='binary')\n",
    "    accuracy = (pred == y_test).mean()\n",
    "    \n",
    "    # Store results\n",
    "    baseline_results[name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall: {recall:.3f}\")\n",
    "    print(f\"  F1-Score: {f1:.3f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nBaseline Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Precision':<10} {'Recall':<10} {'F1':<10}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, metrics in baseline_results.items():\n",
    "    print(f\"{name:<20} {metrics['precision']:<10.3f} {metrics['recall']:<10.3f} {metrics['f1']:<10.3f}\")\n",
    "\n",
    "print(\"=\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac82fd5f-8d8d-4fb0-8fed-fba29231edc6",
   "metadata": {},
   "source": [
    "### 8.4 Results\n",
    "\n",
    "All three default models perform dramatically better than our simple baselines:\n",
    "* Logistic Regression trails but still respectable at 82.7% precision and 63.3% recall\n",
    "* Random Forest leads with 94.1% precision and 81.6% recall - this actually meets our business goals from Step 2\n",
    "* XGBoost close second with 86.7% precision and 79.6% recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a7bee-9946-48f8-9b82-c1739c7b1e88",
   "metadata": {},
   "source": [
    "## Step 8 Conclusion\n",
    "\n",
    "The results show us clearly that PCV features contain crucial fraud signals and the fraud detection problem is solvable. For this project, tree-based models outperform linear models suggesting non-linear fraud patterns.\n",
    "\n",
    "The results of step 8 significantly changed the next steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
