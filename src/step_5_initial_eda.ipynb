{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e01cd2-58b7-4e81-b9ff-cf591b3acc7b",
   "metadata": {},
   "source": [
    "# Step 5 - Initial EDA\n",
    "\n",
    "After the data is acquired, it is time to perform an initial exploratory data analysis with the purpose of data cleaning. In this step, we try to find flaws and problems with the data, and in the next step we will try to rectify them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f908bb3-59c9-4ca5-af47-c9ca7296c053",
   "metadata": {},
   "source": [
    "# 5.1 - Problem: Missing Data\n",
    "\n",
    "One of the most prevalent problems in any dataset is missing data, thus we'll check the number and the percentage of missing data by feature, and based on that we will make a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5a0cf6-04e3-4970-a298-bc52a46e9cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (284807, 31)\n",
      "Dataset size: 8,829,017\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Load data\n",
    "data_path = Path(\"../data/creditcard.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Checking if the data has been loaded successfully by outputing the shape and size\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Dataset size: {df.size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873c465a-2918-49dd-ad2f-052aab9ba679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of missing data per feature:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Printing the number of missing data\n",
    "missing_count = df.isnull().sum()\n",
    "print(\"\\nNumber of missing data per feature:\")\n",
    "print(list(missing_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0ddb1-910d-4a82-b554-cf8896610007",
   "metadata": {},
   "source": [
    "### 5.1 Conclusion\n",
    "\n",
    "We can see that there are no missing data in this dataset and thus we can move on to the next point in our pre-cleaning EDA.\n",
    "\n",
    "In case missing data was present, I would display a distribution of the percentage of missing value per feature to have a better understanding of the problem I'm dealing with and usually would consult a stakeholder and ask for an explanation. Sometimes there's a pattern to the missing values. Sometimes the problem is systematic. And I always check whether we have missing values in the target value when I'm dealing with supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce0ee3-472a-4af2-a064-d7b683ec735d",
   "metadata": {},
   "source": [
    "## 5.2 Problem: Data Type and Format Issues\n",
    "\n",
    "The second most common problem I've faced are:\n",
    "\n",
    "1. Mixed data types within columns.\n",
    "2. Numerical data stored as strings.\n",
    "3. Incorrectly stored data/time values.\n",
    "4. Hidden characters and encoding issues.\n",
    "5. Multiple boolean variations (0/1, T/f, ...).\n",
    "6. Currency or percentage issues.\n",
    "\n",
    "For our project here, we'll first check the datatypes of features. Then we'll check if they match with dataset description. If we see inconsistencies we'll move to the next steps.\n",
    "\n",
    "So, for the ['Kaggle Credit Card Fraud'](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) dataset, the dataset is advertised as:\n",
    "\n",
    "> It contains only numerical input variables which are the result of a PCA transformation.\n",
    "> Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'.\n",
    "> Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset.\n",
    "> The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning.\n",
    "> Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "Thus:\n",
    "\n",
    "1. We'll check whether we have all features by printing the feature names.\n",
    "2. We'll check if V1 to V28 are all numerical.\n",
    "3. We'll check if time feature are numerical.\n",
    "4. We'll check if the Class feature are all 1s and 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "589ce889-6a12-460e-ba06-1c5f20d725cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      "['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n",
      "\n",
      "PCA Features (V1-V28) data types:\n",
      "Unique data types: [dtype('float64')]\n",
      "Number of features: 28\n",
      "\n",
      "Time Feature data type:\n",
      "float64\n",
      "\n",
      "Amount Feature data type:\n",
      "float64\n",
      "\n",
      "Class Feature data type and distribution:\n",
      "Data type: int64\n",
      "Unique values: [0, 1]\n",
      "Value counts:\n",
      "Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Printing the feature names\n",
    "print(\"Feature names:\")\n",
    "print(list(df.columns))\n",
    "\n",
    "selection_mask = ['Time', 'Amount', 'Class']\n",
    "pca_features = df.drop(columns=selection_mask)\n",
    "\n",
    "print(f\"\\nPCA Features (V1-V28) data types:\")\n",
    "print(f\"Unique data types: {pca_features.dtypes.unique()}\")\n",
    "print(f\"Number of features: {len(pca_features.columns)}\")\n",
    "\n",
    "print(\"\\nTime Feature data type:\")\n",
    "print(df['Time'].dtype)\n",
    "\n",
    "print(\"\\nAmount Feature data type:\")\n",
    "print(df['Amount'].dtype)\n",
    "\n",
    "print(\"\\nClass Feature data type and distribution:\")\n",
    "print(f\"Data type: {df['Class'].dtype}\")\n",
    "print(f\"Unique values: {sorted(df['Class'].unique())}\")\n",
    "print(f\"Value counts:\\n{df['Class'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed134d-2b2a-4080-8251-3b37a104c335",
   "metadata": {},
   "source": [
    "### 5.2 Conclusion\n",
    "\n",
    "1. All the features mentioned in Kaggle exists as advertised.\n",
    "2. All PCA features are numerical.\n",
    "3. The Time feature is numerical.\n",
    "4. The Amount feature is numerical.\n",
    "5. The Class feature consists of only 1 and 0.\n",
    "\n",
    "It is not a surprise that the common pre data cleaning EDA results are so far positive since we're using a curated dataset. But I always check nonetheless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59278e89-9e31-475f-938c-f135e80b71ef",
   "metadata": {},
   "source": [
    "## 5.3 Obvious Data Quality Issue\n",
    "\n",
    "The next step is to dig deeper into the quality of the data. I have a checklist of problems that lead to model underperformance or inability to converge that need to dealt with.\n",
    "\n",
    "1. Impossible values: such as a negative age.\n",
    "2. Values being outside of reasonable range.\n",
    "3. Duplicate records.\n",
    "4. Suspicious constant values or patterns.\n",
    "5. Placeholder values.\n",
    "\n",
    "There are more items, but these 5 could apply to our project.\n",
    "\n",
    "Basic statistics (mean, standard deviation, min, max, etc.) are the first tool to answer most of these questions, but we need to display them in an organized way to avoid screen clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1d59ef-8c25-40b4-8932-3606cecb1d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Time Feature Statistics:\n",
      "count    284807.000000\n",
      "mean      94813.859575\n",
      "std       47488.145955\n",
      "min           0.000000\n",
      "25%       54201.500000\n",
      "50%       84692.000000\n",
      "75%      139320.500000\n",
      "max      172792.000000\n",
      "Name: Time, dtype: float64\n",
      "==================================================\n",
      "2. Amount Feature Statistics:\n",
      "count    284807.000000\n",
      "mean         88.349619\n",
      "std         250.120109\n",
      "min           0.000000\n",
      "25%           5.600000\n",
      "50%          22.000000\n",
      "75%          77.165000\n",
      "max       25691.160000\n",
      "Name: Amount, dtype: float64\n",
      "==================================================\n",
      "3. PCA Features (V1-V28) Summary Statistics:\n",
      "Count: 284,807 (same for all PCA features)\n",
      "Mean range: -0.0000 to 0.0000\n",
      "Std range: 0.3301 to 1.9587\n",
      "Min range: -113.7433 to -2.6046\n",
      "Max range: 2.4549 to 120.5895\n",
      "==================================================\n",
      "4. Duplicate Records Check:\n",
      "Number of duplicate rows: 1081\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Basic statistics for Time feature\n",
    "print(\"1. Time Feature Statistics:\")\n",
    "print(df['Time'].describe())\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"2. Amount Feature Statistics:\")\n",
    "print(df['Amount'].describe())\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"3. PCA Features (V1-V28) Summary Statistics:\")\n",
    "pca_stats = pca_features.describe()\n",
    "print(f\"Count: {pca_stats.loc['count'].iloc[0]:,.0f} (same for all PCA features)\")\n",
    "print(f\"Mean range: {pca_stats.loc['mean'].min():.4f} to {pca_stats.loc['mean'].max():.4f}\")\n",
    "print(f\"Std range: {pca_stats.loc['std'].min():.4f} to {pca_stats.loc['std'].max():.4f}\")\n",
    "print(f\"Min range: {pca_stats.loc['min'].min():.4f} to {pca_stats.loc['min'].max():.4f}\")\n",
    "print(f\"Max range: {pca_stats.loc['max'].min():.4f} to {pca_stats.loc['max'].max():.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for duplicate records\n",
    "print(\"4. Duplicate Records Check:\")\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b01939a-cff9-48f3-a3a5-0a7e10233e9c",
   "metadata": {},
   "source": [
    "### 5.3 Initial Conclusion\n",
    "\n",
    "The basic analysis answers some of my questions.\n",
    "\n",
    "1. There are no impossible negative values in Time or Amount.\n",
    "2. There are transactions with amount of zero which need to be investigated.\n",
    "3. There are 1,081 duplicated rows which need to be investigatged.\n",
    "4. PCA features have a mean of zero which means the preprocessing was done properly and we do not need to re-center or re-scale the features which is good.\n",
    "\n",
    "So points 2 and 3 need to be investigated.\n",
    "\n",
    "2. **Zero-amount transactions:** I researched and found no issue with these transactions. Transactions can be zero when coupons are used or are some sort of a test transaction that validates debit/credit cards.\n",
    "3. **Duplicated rows:** There are several logical reasons why duplicate records could exist in the record and are not a concern.\n",
    "    *  After PCA transformation, different original transactions might round to identical values due to floating-point precision limits.\n",
    "    *  Time is recorded in seconds and multiple transactions happening within the same second with the same amount would appear identical.\n",
    "    *  Same person can buy the same item or a recurring payment or common transaction amounts.\n",
    "    *  Common payment processing patterns such as batch processing by merchants, failed transactions that were immediately retried, and authroization holds followed by actual charges.\n",
    "\n",
    "Thus we need to now investigate the duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e62503-1ea1-4ab6-8b45-881aa56e795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1 Analyzing Duplicate Records:\n",
      "Fraud vs Non-fraud in duplicates:\n",
      "Class\n",
      "0    1822\n",
      "1      32\n",
      "Name: count, dtype: int64\n",
      "===================================\n",
      "Time range of duplicates:\n",
      "Min time: 26.0\n",
      "Max time: 172233.0\n",
      "===================================\n",
      "Most common amounts in duplicates:\n",
      "Amount\n",
      "1.00      180\n",
      "150.00     28\n",
      "0.00       28\n",
      "29.00      28\n",
      "0.76       26\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"4.1 Analyzing Duplicate Records:\")\n",
    "\n",
    "# Check if duplicates are in fraud vs non-fraud\n",
    "duplicated_mask = df.duplicated(keep=False)\n",
    "duplicate_df = df[duplicated_mask]\n",
    "\n",
    "print(f\"Fraud vs Non-fraud in duplicates:\")\n",
    "print(duplicate_df['Class'].value_counts())\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Check time distribution of duplicates\n",
    "print(f\"Time range of duplicates:\")\n",
    "print(f\"Min time: {duplicate_df['Time'].min()}\")\n",
    "print(f\"Max time: {duplicate_df['Time'].max()}\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Check if duplicates cluster around certain amounts\n",
    "print(f\"Most common amounts in duplicates:\")\n",
    "print(duplicate_df['Amount'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64826e0e-02f8-46db-8f3c-60c9d1d87c2f",
   "metadata": {},
   "source": [
    "### 5.3 Final Conclusion\n",
    "\n",
    "The results show strong evidence for legitimacy.\n",
    "\n",
    "1. Transaction patterns are realistic. $1 appears 180 times which makes perfect sense.\n",
    "2. Low fraud rate in duplicates which is normal and also could indicate fraudsters using common amount to avoid detection.\n",
    "3. Duplicates are distributed across the span of dataset which is good.\n",
    "4. The other common amounts in duplicates are look realistic.\n",
    "\n",
    "Thus the matter is resolved.\n",
    "\n",
    "If the duplicates were problematic and needed to fixed we needed to run more tests to find a solution and consult stakeholders on the matter. But I'd like to list a few of the common approaches I would take:\n",
    "\n",
    "1. When duplicates are clearly data entry errors, complete removal can be used.\n",
    "2. When the timing is important and for example the most recent transaction are more important I'd only keep the last occurance.\n",
    "3. If duplicates in a subset of fields is not acceptable, we'll create a mask of that subset and drop the corresponding rows with the feature mask value being duplicates.\n",
    "4. Sometimes duplicates in a class are still important and thus we'll only drop duplicates from the other class or classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b6a110-a374-401e-bd2a-ae91b5443104",
   "metadata": {},
   "source": [
    "## 5.4 Structural Issues\n",
    "\n",
    "Sometimes the data has structural issue which usually happens when data is gathered from multiple sources and thus could lead to: Inconsistent column names or ordering, variable schema across different files, misaligned data from joins or merges, header row issues or missing headers, extra or missing columns, and so on.\n",
    "\n",
    "None of these are present in our dataset here so we can skip checking for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56fce16-afbc-42b0-bc1e-6f8ddf0be5fa",
   "metadata": {},
   "source": [
    "## 5.5 Target Variable Quality\n",
    "\n",
    "The last but not least important check before data cleaning is the target variable quality check. Usually I check for:\n",
    "\n",
    "1. Missing target values and whether there's a pattern to them. We have established in this notebook that there is no such problem.\n",
    "2. Target variable distribution and balance problem. Due to the nature of our current project there imbalance is a given and part of the problem and no rectifying steps can be taken in the data cleaning step.\n",
    "3. Inconsistency of target labeling. Again, we have seen that there is no such problem in this project.\n",
    "\n",
    "There is no problem with the target variable quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
