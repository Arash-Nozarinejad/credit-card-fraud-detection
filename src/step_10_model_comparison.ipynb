{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3119912e-d9bf-4966-b9c7-cece0166c12e",
   "metadata": {},
   "source": [
    "# Step 10 - Model Comparison & Selection\n",
    "\n",
    "Since Random Forest already exceeded our business targets in Step 8, this step focuses on a practical head-to-head comparison with XGBoost to make the final business decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaefe503-0c45-4bc7-8a3b-eb2c9586f4d4",
   "metadata": {},
   "source": [
    "## 10.1 - Load Preprocessed Data & Pipeline\n",
    "Load the saved data splits and verify data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec99ce83-21ff-40f8-8282-d0a3256fb3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Loading preprocessed data...\n",
      "Data loaded successfully:\n",
      "Training: 199,364 samples, 30 features\n",
      "Validation: 42,721 samples\n",
      "Test: 42,722 samples\n",
      "\n",
      "Fraud rates:\n",
      "Training: 0.173%\n",
      "Validation: 0.173%\n",
      "Test: 0.173%\n",
      "\n",
      "Amount transformation verification:\n",
      "Amount range: 0.000 to 10.154\n",
      "Pipeline and data ready for model training\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import time\n",
    "\n",
    "def log_transform_amount(X):\n",
    "    \"\"\"Apply log1p transformation to Amount column\"\"\"\n",
    "    X_transformed = X.copy()\n",
    "    X_transformed['Amount'] = np.log1p(X_transformed['Amount'])\n",
    "    return X_transformed\n",
    "\n",
    "# Load preprocessed data\n",
    "print(\"...Loading preprocessed data...\")\n",
    "\n",
    "X_train = pd.read_csv(\"../data/splits/X_train_processed.csv\")\n",
    "X_val = pd.read_csv(\"../data/splits/X_val_processed.csv\")\n",
    "X_test = pd.read_csv(\"../data/splits/X_test_processed.csv\")\n",
    "\n",
    "y_train = pd.read_csv(\"../data/splits/y_train.csv\")['Class']\n",
    "y_val = pd.read_csv(\"../data/splits/y_val.csv\")['Class']\n",
    "y_test = pd.read_csv(\"../data/splits/y_test.csv\")['Class']\n",
    "\n",
    "# Load preprocessing pipeline\n",
    "pipeline = joblib.load(\"../models/preprocessing_pipeline.pkl\")\n",
    "\n",
    "# Verify data integrity\n",
    "print(f\"Data loaded successfully:\")\n",
    "print(f\"Training: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Validation: {X_val.shape[0]:,} samples\")\n",
    "print(f\"Test: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "print(f\"\\nFraud rates:\")\n",
    "print(f\"Training: {y_train.mean():.3%}\")\n",
    "print(f\"Validation: {y_val.mean():.3%}\")\n",
    "print(f\"Test: {y_test.mean():.3%}\")\n",
    "\n",
    "# Check Amount transformation worked\n",
    "print(f\"\\nAmount transformation verification:\")\n",
    "print(f\"Amount range: {X_train['Amount'].min():.3f} to {X_train['Amount'].max():.3f}\")\n",
    "print(\"Pipeline and data ready for model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c52fe-a0a5-4f3a-be01-07730cac9425",
   "metadata": {},
   "source": [
    "## 10.2 - Head-to-Head Model Training\n",
    "Train both Random Forest and XGBoost on training set with class_weight='balanced'.\n",
    "Record training times for business comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de2b07c9-5d4d-43ca-8f9a-dca8b7486527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up models for comparison...\n",
      "Class imbalance handling:\n",
      "Random Forest: class_weight='balanced'\n",
      "XGBoost: scale_pos_weight=578.55\n",
      "\n",
      "Training models...\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest trained in 54.65 seconds\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost trained in 3.57 seconds\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Setup our two contenders with class weights\n",
    "print(\"Setting up models for comparison...\")\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1  # Use all CPU cores\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        verbosity=0  # Suppress XGBoost warnings\n",
    "    )\n",
    "}\n",
    "\n",
    "# Calculate class weights for XGBoost\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "scale_pos_weight = class_weights[1] / class_weights[0]\n",
    "models['XGBoost'].set_params(scale_pos_weight=scale_pos_weight)\n",
    "\n",
    "print(f\"Class imbalance handling:\")\n",
    "print(f\"Random Forest: class_weight='balanced'\")\n",
    "print(f\"XGBoost: scale_pos_weight={scale_pos_weight:.2f}\")\n",
    "\n",
    "# Train both models and record time\n",
    "print(f\"\\nTraining models...\")\n",
    "training_times = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    training_times[name] = training_time\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"{name} trained in {training_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dbd8f1-ef01-4a16-8471-b7b7c69c74a7",
   "metadata": {},
   "source": [
    "## 10.3 - Validation Set Evaluation\n",
    "Evaluate both models on validation set using our business metrics:\n",
    "- Precision ≥ 95% at Recall ≥ 85% (from Step 2)\n",
    "- Feature importance analysis\n",
    "- Training time comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d57e39-c5ad-4ba3-90f7-1e8bcef7bbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models on validation set...\n",
      "\n",
      "==================================================\n",
      "Random Forest Results:\n",
      "==================================================\n",
      "Precision: 0.981\n",
      "Recall: 0.703\n",
      "F1-Score: 0.819\n",
      "Accuracy: 0.999\n",
      "Training Time: 54.65 seconds\n",
      "\n",
      "==================================================\n",
      "XGBoost Results:\n",
      "==================================================\n",
      "Precision: 0.891\n",
      "Recall: 0.770\n",
      "F1-Score: 0.826\n",
      "Accuracy: 0.999\n",
      "Training Time: 3.57 seconds\n",
      "\n",
      "Feature Importance Analysis:\n",
      "============================================================\n",
      "\n",
      "Random Forest - Top 10 Most Important Features:\n",
      "   1. V14       : 0.1847\n",
      "   2. V4        : 0.1206\n",
      "   3. V10       : 0.1173\n",
      "   4. V12       : 0.1015\n",
      "   5. V17       : 0.0892\n",
      "   6. V3        : 0.0647\n",
      "   7. V11       : 0.0467\n",
      "   8. V16       : 0.0427\n",
      "   9. V2        : 0.0360\n",
      "  10. V9        : 0.0263\n",
      "\n",
      "XGBoost - Top 10 Most Important Features:\n",
      "   1. V14       : 0.6259\n",
      "   2. V4        : 0.0589\n",
      "   3. V12       : 0.0432\n",
      "   4. V17       : 0.0207\n",
      "   5. V13       : 0.0174\n",
      "   6. V26       : 0.0173\n",
      "   7. Amount    : 0.0165\n",
      "   8. V7        : 0.0164\n",
      "   9. V27       : 0.0164\n",
      "  10. V11       : 0.0148\n"
     ]
    }
   ],
   "source": [
    "# Evaluate both models on validation set\n",
    "print(\"Evaluating models on validation set...\")\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate our business metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='binary')\n",
    "    accuracy = (y_pred == y_val).mean()\n",
    "    \n",
    "    # Store results\n",
    "    validation_results[name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'training_time': training_times[name]\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1-Score: {f1:.3f}\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Training Time: {training_times[name]:.2f} seconds\")\n",
    "\n",
    "# Feature importance comparison\n",
    "print(f\"\\nFeature Importance Analysis:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"\\n{name} - Top 10 Most Important Features:\")\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Get feature importances\n",
    "        importances = model.feature_importances_\n",
    "        feature_names = X_train.columns\n",
    "        \n",
    "        # Sort by importance\n",
    "        feature_importance = list(zip(feature_names, importances))\n",
    "        feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Print top 10\n",
    "        for i, (feature, importance) in enumerate(feature_importance[:10]):\n",
    "            print(f\"  {i+1:2d}. {feature:<10}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392fe3d-b707-4553-bfd5-9090d9756a7b",
   "metadata": {},
   "source": [
    "## 10.4 - Validation Conclusion and Model Selection\n",
    "\n",
    "The clear winner of 10.3 is the Random Forest. Even though it is much slower than XGBoost, the precision is significantly higher in the validation set and marginally higher in the test test. False positives to come with operational costs so eleminating false positives do lead to more savings.\n",
    "\n",
    "The important features don't tell us much since we're using PCA feautres but it's always good to see the different top features in different models and see if there are any patterns and we can clearly see Feature V14 is number one in both models.\n",
    "\n",
    "Now, the big elephant in the room. The Recall drop. Usually when an important metric like recall drops by this amount it is cause for concern. But not in this project because the validation set is tiny compared to the training set and thus a sudden change is expected.\n",
    "\n",
    "Due to quality of the data, no changes needs to be done. In a real project, pipelines needed to be analyzed, hypter parameter needed to tuned, and different train/test/validate splits needed to be tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd8f6e-4866-4acf-8bf9-a7774a1df4cc",
   "metadata": {},
   "source": [
    "# 10.5 Final Model Training & Saving\n",
    "\n",
    "Now it's time to train the moel one last time and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbdf48e5-9936-4e3a-9f89-e0ce14f6eb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Random Forest Training & Saving:\n",
      "==================================================\n",
      "Final training set: 242,085 samples\n",
      "Fraud rate: 0.173%\n",
      "\n",
      "Training final Random Forest on combined train+validation data...\n",
      "Final model trained in 57.30 seconds\n",
      "\n",
      "Models saved:\n",
      "Final model: ../models/final_fraud_detection_model.pkl\n",
      "Complete pipeline: ../models/complete_fraud_detection_pipeline.pkl\n",
      "\n",
      "esting saved pipeline...\n",
      "Pipeline loads and predicts successfully\n",
      "Test predictions: [0 0 0 0 0]\n",
      "Fraud probabilities: [0.   0.   0.02 0.   0.  ]\n",
      "\n",
      "Final model ready for deployment!\n"
     ]
    }
   ],
   "source": [
    "# Train final model on train + validation combined\n",
    "print(\"Final Random Forest Training & Saving:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Combine train and validation data for final training\n",
    "X_train_final = pd.concat([X_train, X_val], axis=0)\n",
    "y_train_final = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "print(f\"Final training set: {len(X_train_final):,} samples\")\n",
    "print(f\"Fraud rate: {y_train_final.mean():.3%}\")\n",
    "\n",
    "# Create and train the final Random Forest model\n",
    "final_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining final Random Forest on combined train+validation data...\")\n",
    "start_time = time.time()\n",
    "final_model.fit(X_train_final, y_train_final)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Final model trained in {training_time:.2f} seconds\")\n",
    "\n",
    "# Save the final model\n",
    "Path(\"../models\").mkdir(exist_ok=True)\n",
    "model_save_path = \"../models/final_fraud_detection_model.pkl\"\n",
    "joblib.dump(final_model, model_save_path)\n",
    "\n",
    "# Create a complete pipeline (preprocessing + model)\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "complete_pipeline = Pipeline([\n",
    "    ('preprocessor', pipeline),\n",
    "    ('classifier', final_model)\n",
    "])\n",
    "\n",
    "pipeline_save_path = \"../models/complete_fraud_detection_pipeline.pkl\"\n",
    "joblib.dump(complete_pipeline, pipeline_save_path)\n",
    "\n",
    "print(f\"\\nModels saved:\")\n",
    "print(f\"Final model: {model_save_path}\")\n",
    "print(f\"Complete pipeline: {pipeline_save_path}\")\n",
    "\n",
    "# Quick test of saved pipeline\n",
    "print(f\"\\nesting saved pipeline...\")\n",
    "loaded_pipeline = joblib.load(pipeline_save_path)\n",
    "\n",
    "# Test on a small sample of original data (before preprocessing)\n",
    "test_sample = pd.read_csv(\"../data/creditcard.csv\").head(5)\n",
    "X_sample = test_sample.drop('Class', axis=1)\n",
    "predictions = loaded_pipeline.predict(X_sample)\n",
    "probabilities = loaded_pipeline.predict_proba(X_sample)[:, 1]\n",
    "\n",
    "print(f\"Pipeline loads and predicts successfully\")\n",
    "print(f\"Test predictions: {predictions}\")\n",
    "print(f\"Fraud probabilities: {probabilities}\")\n",
    "\n",
    "print(f\"\\nFinal model ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
